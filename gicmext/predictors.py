#!/usr/bin/env python

## Copyright (C) 2005-2006 Graham I Cummins
## This program is free software; you can redistribute it and/or modify it under 
## the terms of the GNU General Public License as published by the Free Software 
## Foundation; either version 2 of the License, or (at your option) any later version.
## 
## This program is distributed in the hope that it will be useful, but WITHOUT ANY 
## WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
## PARTICULAR PURPOSE. See the GNU General Public License for more details.
## 
## You should have received a copy of the GNU General Public License along with 
## this program; if not, write to the Free Software Foundation, Inc., 59 Temple 
## Place, Suite 330, Boston, MA 02111-1307 USA
## 

from mien.datafiles.dataset import *
import numpy.linalg as lam
import sysChar as sc
import calibration as cal
import gicmext.dimred as dimr

	
def pca(ds, dpath="/evtcond", newpath="/pca", howMuchVar=.7, inverse=False):
	'''Calculates principal components of a set of vectors stared in dpath (must be 2D NxM). Stores a 2D array in newpath. This matrix contains, in the first column, the mean, and in the remaining colums, the principal eigenvectors. howMuchVar determines how many eigenvectors to record. If this number is >=1, all eigenvectors are recorded, otherwise enough components to explain >= this fraction of the total variance (around the mean) are recorded. If Inverse is true, the least significant PCs are recorded first, and "howMuchVar" of the varriance is left unexplained. The labels of the 
	eigenvector channels contain the corresponding eigenvalue.
	
	SWITCHVALUES(mode)=[True, False]
	'''
	dati=ds.getSubData(dpath)
	dat=dati.getData()
	pc0=mean(dat, 1)
	cv=cov(dat)
	#cov(dat) = dot(m, transpose(m))/(dat.shape[1] - 1), where m = dat - dat.mean(1)[:,newaxis]
	val, vec=lam.eig(cv)
	val=val/val.sum()
	howmany=val.shape[0]
	if howMuchVar<1.0:
		howmany=nonzero1d(cumsum(val)>=howMuchVar)[0]
		if inverse:
			howmany=val.shape[0]-howmany
	eigs=zeros((vec.shape[0],howmany), dat.dtype)
	labs=[0]*howmany
	order=argsort(val)
	if not inverse:
		order=reverseArray(order)
	for i in range(howmany):
		nth=order[i]
		eigs[:,i]=vec[:,nth]
		labs[i]=float(val[i])
	print labs
	labs=['mean']+labs
	eigs=concatenate([pc0[:,NewAxis], eigs], 1)
	head={'SampleType':'timeseries', 'SamplesPerSecond':dati.fs(), 'Labels':labs} 
	ds.createSubData(newpath, eigs, head, True)
	return ds
	
def pcaKernel(ds, select=(None, None, None), dpath='/pca', newpath='/pcreject'):
	'''Applies a PC-rejection distance measure to the data in select, using the mean and principal components in dpath (as generated by the pca function in this module). The result is stored in newpath. '''
	comps=ds.getSubData(dpath).getData()
	stim=getSelection(ds, select)
	sp=sqrt((stim**2).sum())
	#comps[:,0]*=sp
	#print sp
	out=sc.pcafilt(stim[:,0], comps)
	#out[:comps.shape[0]]=out[comps.shape[0]:].max()
	out=-out/(out.max()-out.min())
	out-=out.min()
	h=getSelectionHeader(ds, select)
	ds.createSubData(newpath, out, h, True)
	return ds



def projectionAnalyzer(ds, dpathUE="/UE", dpathECE='/evtcond', dpathFilters='/pca', bins=60, smooth=0, newpath='/hists'):
	'''construct the de-biased distribution of conditioned projections onto a filter or set of filters. The filters must have the same length as the conditioned events in dpathConditional. The distributions are calculated as histograms in bins of width "step"'''
	raw=ds.getSubData(dpathUE).getData()
	cond = ds.getSubData(dpathECE).getData()
	comps=ds.getSubData(dpathFilters).getData()
	hists=zeros((bins, comps.shape[1]), float32)
	ranges=[]
	for v in range(comps.shape[1]):
		filt=comps[:,v]
		cp=dot(filt, cond)
		rp=dot(filt, raw)
		st=min([rp.min(), cp.min()])
		top=max([cp.max(), rp.max()])
		bw=(top-st)/float(bins)
		ranges.append((st, bw))
		rp=hist2(rp, bw, st, bins).astype(float32)/raw.shape[1]
		cp=hist2(cp, bw, st, bins).astype(float32)/cond.shape[1]
		z = arange(rp.shape[0])
		if smooth:
			rp = dimr._lstsqO4(rp, z)[1]
			cp = dimr._lstsqO4(cp, z)[1]
		mask=nonzero1d(rp==0)
		put(rp, mask, 1)
		put(cp, mask, 0.0)
		hists[:,v]=cp/rp
	h=ds.header()
	h['Labels']=ds.getSubData(dpathFilters).getLabels()
	h['BinWidths']=[q[1] for q in ranges]
	h['StartBins']=[q[0] for q in ranges]
	ds.createSubData(newpath, data=hists, head=h, delete=True)
	return ds
			
def projectionModel(ds, dpathInput='/', dpathFilters='/pca', dpathHists='/hists', newpath='/dp'):
	'''Get a direct measure of projection proability'''
	dat=ds.getSubData(dpathInput).getData()
	comps=ds.getSubData(dpathFilters).getData()
	hists=ds.getSubData(dpathHists)
	sbs=hists.attrib('StartBins')
	bws=hists.attrib('BinWidths')
	hists=hists.getData()
	pred=zeros_like(dat)
	fl = comps.shape[0]
	for i in range(fl,dat.shape[0]):
		s = dat[i-fl:i,0]
		for v in range(comps.shape[1]):
			p = dot(s, comps[:,v])
			targ=hists[:,v]
			bin = int(round( (p - sbs[v])/bws[v]))
			if bin < 0:
				bin = 0
			if bin >= targ.shape[0]:
				bin = targ.shape[0]-1
			pred[i]+=targ[bin]
	h=ds.getSubData(dpathInput).header()
	ds.createSubData(newpath, data=pred, head=h, delete=True)
	return ds
	
def histToMoments(ds, dpath):
	pass

def setTestData(ds, xindexStart=70, xindexStop=100, percent=True):
	'''Sets all data between the indicated indexes for use as validation data. These data are extracted from "/" and stored in "/testdata". In addition, for every "events" or "labeledevents" element at nesting level 1 the same set of indexes are extracted and stored in sub-elements of "/testdata". Essentially, this is a macro around 2 or more calls to subdata.extract. If percent is set to true, the values of the xindex parameters are treated as percentages of the total length of the data rather than integer indexes.
	
	SWITCHVALUES(percent)=[True, False]'''
	if percent:
		l=ds.getData().shape[0]
		xindexStart=int(l*xindexStart/100.0)
		if xindexStop in [':', None, 100]:
			xindexStop=l
		else:	
			xindexStop=int(l*xindexStop/100.0)
	import mien.dsp.subdata as sd
	sd.extract(ds, (None, None, (xindexStart, xindexStop)), "/testdata", True)
	d1d=ds.getElements('Data', depth=1)
	e1d=[d for d in d1d if isSampledType(d)=='e']
	for e in e1d:
		op=e.dpath()
		np="/testdata"+op
		sd.extract(ds, (op, None, (xindexStart, xindexStop)), np, True)
	return ds	
	
def getTestData(ds):
	'''Gets the data (and events) stored in "/testdata" and moves them to the toplevel. Moves toplevel data and events to "/testdata". (for clairity it wolud be appropriate to rename this to "/trainingdata", but keeping the name constant allows this function to toggle the contents of the top level and /testdata.)
	In general this functino should only be used after setTestData.'''
	import mien.dsp.subdata as sd
	sd.moveToTop(ds, '/testdata', True)
	d1d=ds.getElements('Data', depth=1)
	e1d=[d for d in d1d if isSampledType(d)=='e']
	td=ds.getSubData('/testdata')
	dtd=td.getElements('Data', depth=1)
	etd=[d for d in dtd if isSampledType(d)=='e']
	for e in e1d:
		e.move(td)
	for e in etd:
		e.move(ds)
	return ds	

def Filter(ds, select=(None, None, None), selectFilter=('/pca', [0], None), eventOffset=-.02, newpath='/dp'):
	'''Apply the data specified by selectFilter as a filter on the data specified by select. Store the resulting signal in newpath.'''
	filt=getSelection(ds, selectFilter)[:,0]
	#filt = reverseArray(filt[:,0])
	dat=getSelection(ds, select)[:,0]
	dat = cal._applyFilterWithTR(dat, filt)
	h=getSelectionHeader(ds, select)
	fs = h['SamplesPerSecond']
	fl = filt.shape[0] / fs
	os = (fl / 2) + eventOffset
	h['StartTime']+=os
	ds.createSubData(newpath, data=dat, head=h, delete=True)
	return ds
		
		
def corseAlign(ds, dpathDat="/nl", dpathEvts='/spikes', minshift=.015, maxshift=.015, offset=0.00):
	'''align events from an event set in dpath with the signal in select such that the signal is maximal at the time of the events. Maxshift is the largest shift (in seconds) allowed. A single shift is applied to the whole event set'''
	nds=ds.getSubData(dpathDat)
	start=nds.start()
	fs=nds.fs()
	evtset=ds.getSubData(dpathEvts)
	evts=evtset.getData(copy=True)
	dat = nds.getData()[:,0]
	if not fs==evtset.fs():
		raise StandardError("sample rate not compatible")
	if not start==evtset.start():
		td=evtset.start()-start
		evts+=round(td*fs)
	if offset:
		evts+=round(offset*fs)		
	ms=round(maxshift*fs)
	mis=round(minshift*fs)
	mask=logical_and(evts>mis, evts<dat.shape[0]-ms-1)
	mask=nonzero1d(mask)
	if mask.shape[0]<evts.shape[0]:
		evts=take(evts, mask)
	shifts=arange(-mis, ms+1)
	vals=zeros(shifts.shape, float64)
	for i, s in enumerate(shifts):
		vals[i]=take(dat, (evts+s).astype(int32)).sum()
	#print vals
	os=shifts[argmax(vals)]
	evts=(evts+os).astype(int32)
	otime = -float(os)/fs
	print "optimal shift is %.5f" % (otime, )
	print "raw mean %.3f, conditioned mean %.3f" % (dat.mean(), take(dat, evts).mean())
	start = start + otime
	nds.setAttrib("StartTime", start)
	
def victorDistance(ds, dpathPred='/gspikes', dpathDat="/spikes", cost=.01, searchOffset=20):
	'''calculate the victor/purpura spike metric between the two spike sets, with the indicated cost'''
	if not searchOffset:
		d1 = ds.getSubData(dpathPred).getData()[:,0]
		d2 = ds.getSubData(dpathDat).getData()[:,0]
		dist = sc.victorDistance(d1, d2, cost)
		print dist
	else:
		offsets = range(-searchOffset, searchOffset)
		distances = []
		d1 = ds.getSubData(dpathPred).getData()[:,0]
		d2 = ds.getSubData(dpathDat).getData()[:,0]
		for i, o in enumerate(offsets):
			td = d1 + o
			dist = sc.victorDistance(td, d2, cost)
			distances.append(dist)
		mdi = distances.index(min(distances))
		print "Offset: %i, Distance: %g" % (offsets[mdi], distances[mdi])
		dist = distances[mdi]
	z = ds.getSubData(dpathDat).getData().shape[0]
	r = ds.attrib('real_victordist')
	r = r * z
	s = dist - r
	print z, r
	s = 100 - 100*s / (z-r)
	print "percentage = %g" % s
			
	

def orderPCsByProjection(ds, dpathPC='/pca', dpathHist="/hists", keep=0, newpath="/opc"):
	pcs = ds.getSubData(dpathPC)
	hists = ds.getSubData(dpathHist).getData()
	powers = []
	print "hi"
	for i in range(hists.shape[1]):
		h = hists[:,i]
		p = sqrt( ((h-h.mean())**2).sum())
		#z = arange(h.shape[0])
		#rp = dimr._lstsqO4(h, z)[0]
		#print rp
		#p = h.std()
		powers.append(p)
	order = argsort(-array(powers))
	if keep:
		order = order[:keep]
	print order
	h = pcs.header()
	lab = [h["Labels"][i] for i in order]
	h["Labels"]=lab
	dat = pcs.getData()[:,order]
	ds.createSubData(newpath, data=dat, head=h, delete=True)
		

def randomVictorDistance(ds, dpathPred='/gspikes', dpathDat="/spikes", cost=.01, searchOffset=20):
	rd = []
	d1 = ds.getSubData(dpathPred).getData()[:,0]
	d2 = ds.getSubData(dpathDat).getData()[:,0]
	ran = (min(min(d2), min(d1)), max(max(d2), max(d1)))
	nspikes = d1.shape[0]
	for j in range(20):
		offsets = range(-searchOffset, searchOffset)
		distances = []
		d1 = randint(ran[0], ran[1], nspikes)
		for i, o in enumerate(offsets):
			td = d1 + o
			dist = sc.victorDistance(td, d2, cost)
			distances.append(dist)
		mdi = distances.index(min(distances))
		rd.append(distances[mdi])
	rd = array(rd)
	print "Mean %g, Std %d" % (mean(rd), std(rd))
	

def waveformMatch(ds, dpathPred='/dp', dpathDat="/spikes"):
	d1 = ds.getSubData(dpathPred)
	s1 = d1.start()
	d2 = ds.getSubData(dpathDat)
	s2 = d2.start()
	d2 = d2.getData()[:,0]
	if not s2==s1:
		cor = int(round((s1-s2)*d1.fs()))
		d2 = d2 - cor
	d1 = d1.getData()[:,0]
	hit = d1[d2]
	d3 = setdiff1d(arange(d1.shape[0]), unique1d(d2))
	miss = d1[d3]
	hit = hit.mean()
	miss = miss.mean()
	s = d1.std()
	print "hit mean %g, miss mean %g, std %g, score %g" % (hit, miss, s, (hit - miss)/s)
	s = (hit - miss)/s
	r = ds.attrib('real_wavematch')
	s = 100* s/4.94
	print "percentage = %g" % s
		
	
def gaussianActivation(ds, dpathEnsemble="/evtcond", dpathInput="/", newpath="/dp"):
	ens = ds.getSubData(dpathEnsemble)
	dat = ens.getData()
	me = dat.mean(1)
	co = cov(dat)
	inp = ds.getSubData(dpathInput)
	fs = inp.fs()
	start = inp.start()
	inp = inp.getData()
	out = zeros(inp.shape[0])
	l = me.shape[0]
	norm = 1.0 / ( sqrt(linalg.det(co))*(2*pi)**(l/2.0))
	icov = linalg.inv(co)
	for i in range(l, out.shape[0]):
		x = inp[i-l:i,0] - me
		out[i] = -.5*dot(dot(x, icov), x)
	#out = norm*exp(out)
	h = {"SampleType":"timeseries", "SamplesPerSecond":fs, "StartTime":start}
	ds.createSubData(newpath, data=out, head=h, delete=True)
	
def logLikelihoodRatio(ds, dpathEnsemble="/evtcond", dpathNullEnsemble="/UE", dpathInput="/", newpath="/dp"):
	ens = ds.getSubData(dpathEnsemble)
	dat = ens.getData()
	me = dat.mean(1)
	co = cov(dat)
	inp = ds.getSubData(dpathInput)
	fs = inp.fs()
	start = inp.start()
	inp = inp.getData()
	out = zeros(inp.shape[0])
	l = me.shape[0]
	icov = linalg.inv(co)
	raw = ds.getSubData(dpathNullEnsemble).getData()
	rme = raw.mean(1)
	rco = cov(raw)
	irco = linalg.inv(rco)
	for i in range(l, out.shape[0]):
		x = inp[i-l:i,0] - me
		act = -.5*dot(dot(x, icov), x)
		rx = inp[i-l:i,0] - rme
		ract = -.5*dot(dot(rx, irco), rx)
		out[i] = act - ract
	out[:l] = out.min()
	#out = norm*exp(out)
	h = {"SampleType":"timeseries", "SamplesPerSecond":fs, "StartTime":start}
	ds.createSubData(newpath, data=out, head=h, delete=True)		

def calcWiener2(ds, dpathInput="/", dpathEnsemble='/evtcond', newpath='/kernel'):
	'''Calculates the 0, 1, and 2 order wiener kernels between the Input and Event channelis.'''
	dat=ds.getSubData(dpathEnsemble).getData()
	sd = ds.getSubData(dpathInput)
	inp = sd.getData()
	h0=float(dat.shape[1])/inp.shape[0]
	sigma2=(inp**2).sum()/inp.shape[0]
	sigma4=(inp**4).sum()/inp.shape[0]
	h1=dat.mean(1)/sigma2
	h1=h1-h1.mean()
	h2=dot(dat, transpose(dat))/(dat.shape[1]-1)
	h2=h2/(sigma4)
	h2=h2-identity(h2.shape[0])*h0/sigma2
	ds.createSubData(newpath, data=None, head={"SampleType":"group", "h0":h0}, delete=True)
	ds.createSubData(newpath+"/h1", data=h1, head={"SampleType":"timeseries", "SamplesPerSecond":sd.fs()}, delete=True)
	ds.createSubData(newpath+"/h2", data=h2, head={"SampleType":"timeseries", "SamplesPerSecond":sd.fs()}, delete=True)
	
def applyWiener2(ds, dpathKernel='/kernel', dpathInput="/", newpath='/dp'):
	'''generate a prediction by applying the 0, 1, and 2 order Wiener kernels, as generated by 
	calcWiener2, stored in dpathKernel'''
	ker = ds.getSubData(dpathKernel)
	h0 = ker.attrib("h0")
	h1 = ker.getSubData('h1').getData()
	h2 = ker.getSubData('h2').getData()
	stim=ds.getSubData(dpathInput)
	fs = stim.fs()
	start = stim.start()
	stim = stim.getData()[:,0]
	stim=concatenate((zeros(h1.shape[0]), stim))
	kl=h1.shape[0]
	cor=int(round(h1.shape[0]/2.0))
	y1=convolve(stim, reverseArray(h1), mode='same')[cor:]
	y1=y1[:stim.shape[0]-kl]	
	try:
		y2=sc.apply2dkern(stim, h2)
		y2=y2[h1.shape[0]:]
	except:
		print "sysChar module 2Dkern method failed"
		try:
			stimM=sequential_windows(stim, h1.shape[0])
			y2=sum( stimM*(matrixmultiply(stimM, h2) ), 1)
		except MemoryError:
			print "data too long for array method Iterating. Will be slow"
			y2=zeros(stim.shape[0]-kl, stim.dtype.char)
			print stim.shape[0]
			for ind in range(kl, stim.shape[0]):
				s=stim[ind-kl:ind]
				v=dot(dot(transpose(s),h2),s)
				y2[ind-kl]=v
	#resp= y1+.01*y2
	resp = column_stack([y1, y2])
	ind=ds.createSubData(newpath, data=resp, head={"SampleType":"timeseries", "SamplesPerSecond":fs, "StartTime":start}, delete=True)

def combineWiener(ds, dpathPred="/dp", ratio=.02):
	sd = ds.getSubData(dpathPred)
	dat = sd.getData()
	dat = dat[:,0] + ratio*dat[:,1]
	sd.datinit(dat)

# def simpleDiscriminator(ds, cindexInput=0, cindexOutput=-1, sKey="Kernel", filterChannel=0):
# 	filt=ds.special[sKey]
# 	filt=filt.data[:,filterChannel]
# 	tdat=sc.match(ds.data[:,cindexInput],filt)
# 	tdat= -tdat
# 	ind=ds.assignChannel(cindexOutput, tdat)
# 	ds.labels[ind]="Disc_%s" % sKey	
# 	return ds
# 
# 
# def varianceKernel(ds, normAmp=False, cindexInput=0, cindexOutput=-1, sKey="Kernel"):
# 	filt=ds.special[sKey]	
# 	dat=sc.invar(ds.data[:,cindexInput],filt.data[:,0], filt.data[:,1], normAmp)
# 	dat=(dat/(dat.max()-dat.min()))-dat.min()
# 	ind=ds.assignChannel(cindexOutput, dat)
# 	return ds
# 


# 
# def distribOfProjectedStimuli(ds, cindexStim=0, sKeyEnsemble='EvtCond', sKeyPCA='Components', bins=60):
# 	'''Calculates the distribution of projections of the stimuli in the ensemble onto the 
# 	principa/ components in PCA'''
# 	pcs=ds.special[sKeyPCA]
# 	comp=pcs.data
# 	ens=ds.special[sKeyEnsemble].data
# 	#m=mean(ens, 1)
# 	m=comp[:,0]
# 	ens=ens-m[:,NewAxis]
# 	ucens=ds.getChannel(cindexStim)	
# 	ucens=transpose(sequential_windows(ucens, comp.shape[0]))
# 	ucens=ucens-m[:,NewAxis]
# 	npcs=comp.shape[1]-1
# 	out=zeros([bins, npcs], Float32)
# 	for i in range(npcs):
# 		proj=dot(comp[:,i+1], ens)
# 		p=dot(comp[:,i+1], ucens)
# 		m, std = [p.mean(), p.stddev()]
# 		h=histogram(proj, bins=bins)
# 		debias=exp(-.5*(h[:,0]-m)**2/std**2)
# 		h=h[:,1]/debias
# 		out[:,i]=h/h.sum()
# 	new=DataSet(out)
# 	ds.special['ProjDistrib']=new
# 	return ds
# 	
# 
# def calcWiener2(ds, cindexInput=0, cnameEvents="EventTimes", length=.020, lead=.020, sKey='Kernel'):
# 	'''Calculates the 0, 1, and 2 order wiener kernels between the Input and Event channels.
# 	Assumes "Events" to be discrete (0 or 1).'''
# 	rast=ds.getChannel(cnameEvents)
# 	thresh = (rast.max()-rast.min())/2.0
# 	ind = nonzero(rast>thresh)
# 	ind-=int(round(lead*ds.fs))
# 	length=int(round(length*ds.fs))
# 	dat=transpose(ds.takeWindows(cindexInput, ind, length))
# 	inp=ds.getChannel(cindexInput)
# 	h0=rast.mean()
# 	sigma2=(inp**2).sum()/inp.shape[0]
# 	sigma4=(inp**4).sum()/inp.shape[0]
# 	h1=sum(dat, 0)/(dat.shape[0]-1)
# 	h1=h1/sigma2
# 	h1=h1-h1.mean()
# 	h2=dot(transpose(dat), dat)/(dat.shape[0]-1)
# 	h2=h2/(sigma4)
# 	h2=h2-identity(h2.shape[0])*h0/sigma2
# 	ds.special[sKey]=[h0, h1, h2]
# 	#print length, dat.shape, h2.shape, h1.shape
# 	#ds.data=concatenate([h1[:,NewAxis], h2], 1)
# 	return ds
# 	
# def applyWiener2(ds, cindexInput=0, cindexOutput=-1, sKey='Kernel'):
# 	'''generate a prediction by applying the 0, 1, and 2 order Wiener kernels, as generated by 
# 	calcWiener2, stored in sKey to cindexInput'''
# 	h0, h1, h2=ds.special[sKey]
# 	stim=ds.getChannel(cindexInput)
# 	stim=concatenate((zeros(h1.shape[0]), stim))
# 	kl=h1.shape[0]
# 	cor=int(round(h1.shape[0]/2.0))
# 	y1=convolve(stim, reverseArray(h1), mode=SAME)[cor:]
# 	y1=y1[:stim.shape[0]-kl]	
# 	try:
# 		y2=sc.apply2dkern(stim, h2)
# 		y2=y2[h1.shape[0]:]
# 	except:
# 		print "sysChar module 2Dkern method failed"
# 		raise
# 		try:
# 			stimM=sequential_windows(stim, h1.shape[0])
# 			y2=sum( stimM*(matrixmultiply(stimM, h2) ), 1)
# 		except MemoryError:
# 			print "data too long for array method Iterating. Will be slow"
# 			y2=zeros(stim.shape[0]-kl, stim.dtype.char)
# 			print stim.shape[0]
# 			for ind in range(kl, stim.shape[0]):
# 				s=stim[ind-kl:ind]
# 				v=dot(dot(transpose(s),h2),s)
# 				y2[ind-kl]=v
# 	resp= h0+y1+y2
# 	ind=ds.assignChannel(cindexOutput, resp)
# 	ds.labels[ind]="Wiener2_%s" % sKey
# 	return ds
# 	
# def pcaKernel(ds, cindexInput=0, cindexOutput=-1, sKey="Components"):
# 	'''pca filter'''
# 	comps=ds.special[sKey].data.copy()
# 	stim=ds.data[:,cindexInput]
# 	sp=sqrt((stim**2).sum())
# 	#comps[:,0]*=sp
# 	#print sp
# 	out=sc.pcafilt(stim, comps)
# 	#out[:comps.shape[0]]=out[comps.shape[0]:].max()
# 	out=-out/(out.max()-out.min())
# 	out-=out.min()
# 	ind=ds.assignChannel(cindexOutput, out)
# 	ds.labels[ind]="PCAFilter_%s" % sKey
# 	return ds
# 
# def rspaceProb(ds, cindexInput=0,cindexOutput=-1, sKey="isidist"):
# 	'''Test the data in cindexInput for consistancy with the spike
# 	isi distributions present in isidist.'''
# 	isidist=ds.special['isidist']
# 	psib=isidist.special['psib']
# 	pd=isidist.data[:2, :]
# 	dat=isidist.data[2:,:]
# 	dur=dat.shape[0]/isidist.fs
# 	cids={}
# 	probs={}
# 	probs[0]=1-pd[1,:].sum()
# 	for i in range(pd.shape[1]):
# 		ns=pd[0,i]
# 		probs[ns]=pd[1,i]
# 		cids[ns]=i
# 	inp=ds.getChannel(cindexInput)
# 	spb=int(round(ds.fs/isidist.fs))
# 	nwin, remain=divmod(inp.shape[0],spb)
# 	inp=inp[remain:]
# 	inp=reshape(inp, (nwin, spb))
# 	inp=sum(inp, 1)
# 	out=zeros(inp.shape[0], ds.data.dtype.char)
# 	histlength=dat.shape[0]
# 	inp=concatenate([zeros(histlength, inp.dtype.char), inp])
# 	for i in range(nwin):
# 		his=inp[i:i+histlength]
# 		ns=his.sum()
# 		if ns==0:
# 			#out[i]=probs[0]
# 			out[i]=psib
# 		elif not probs.has_key(ns):
# 			out[i]=0.0
# 		else:	
# 			p=probs[ns]
# 			v=take(dat[:,cids[ns]], nonzero(his))
# 			v=multiply.reduce(v)*p
# 			out[i]=v	
# 	out=resize(out, (spb, out.shape[0]))
# 	out=reshape(transpose(out), -1)
# 	if out.shape[0]!=ds.data.shape[0]:
# 		print ds.data.shape, out.shape
# 		fill=ones(ds.data.shape[0]-out.shape[0], out.dtype.char)*probs[0]
# 		out=concatenate([fill, out])
# 	ds.setChannel(cindexOutput, out)
# 	return ds
# 
# def pcaProjector(ds, cindexInput=0, cindexOutput=-1, sKey="Components"):
# 	'''pca filter'''
# 	comps=ds.special[sKey].data.copy()
# 	stim=ds.data[:,cindexInput]
# 	sp=sqrt((stim**2).sum())
# 	#comps[:,0]*=sp
# 	#print sp
# 	out=sc.pcaproj(stim, comps)
# 	#out[:comps.shape[0]]=out[comps.shape[0]:].max()
# 	out=max(out)-out
# 	ind=ds.assignChannel(cindexOutput, out)
# 	ds.labels[ind]="PCAFilter_%s" % sKey
# 	return ds
# 
# def summedComponentProjection(ds, cindexInput=0, cindexOutput=-1, sKey="Components"):
# 	'''pca filter'''
# 	comps=ds.special[sKey].data.copy()
# 	stim=ds.data[:,cindexInput]
# 	sp=sqrt((stim**2).sum())
# 	#comps[:,0]*=sp
# 	#print sp
# 	allcomps=sum(comps[:,1:], 1)
# 	comps=comps[:,:2]
# 	comps[:,1]=allcomps
# 	out=sc.pcaproj(stim, comps)
# 	#out[:comps.shape[0]]=out[comps.shape[0]:].max()
# 	out=max(out)-out
# 	ind=ds.assignChannel(cindexOutput, out)
# 	ds.labels[ind]="PCAFilter_%s" % sKey
# 	return ds
# 
# 
# 
# def refractoryPoisson(ds, cindexGenerator=0, nreps=1, threshold=.5, max=1.0, refractMag=.5, refractTao=.002, addResults=False):
# 	'''generate a random sequence ranging from threshold to max. If Generator is 
# 	greater than this value, create an event. Modify the event sequence to include
# 	refractory effects as follows:
# 	For ease of optimization, threshold and refractMag are expressed as fractions of
# 	max, so, e.g. if max = 2.0 and threshold = .5 a threshold of 1 is used.
# 	For each event, reduce the generator by refractMag*exp(-dt/refractTao) where dt
# 	is the time (seconds) since the last event. If this reduces G below the random value,
# 	remove the event.
# 	repeat the procedure nreps times, adding a new channel for each set of spikes.
# 	If nreps is greater than one, and addResults is true, produce a single output channel 
# 	that is the sum of nreps separate realizations.'''
# 	dp=ds.getChannel(cindexGenerator)
# 	threshold=max*threshold
# 	refractMag=max*refractMag
# 	for rep in range(nreps):
# 		thresh=uniform(threshold, max, dp.shape)
# 		evts=nonzero(dp>thresh)
# 		if len(evts)>0:
# 			new=[evts[0]]
# 			ts=refractTao*ds.fs
# 			diffs=take(dp-thresh, evts)
# 			for ind in range(1, len(evts)):
# 				dt=evts[ind]-new[-1]
# 				pen=refractMag*exp(-dt/ts)
# 				if diffs[ind]>pen:
# 					new.append(evts[ind])
# 			evts=array(new)		
# 			new=zeros(dp.shape, dp.dtype.char)
# 			put(new, evts, 1)
# 		else:
# 			new=zeros(dp.shape, dp.dtype.char)
# 		print int(new.sum())	
# 		if rep==0 or not addResults:
# 			ochannelindex=ds.setChannel(-1, new)
# 			ds.labels[ochannelindex]="rp_%i_%i" % (cindexGenerator, rep)
# 		else:
# 			ds.data[:,ochannelindex]+=new
# 			
# 	return ds
# 
# def testPrediction(ds, cnameEvents="Events", cindexPrediction=-1, offset='auto', window=0.0):
# 	'''Compare the prediction in cindexPrediction to the events in the named channel.'''
# 	if cindexPrediction<0:
# 		cindexPrediction=ds.data.shape[1]+cindexPrediction
# 	evts=nonzero(ds.getChannel(cnameEvents))
# 	pred=ds.channel(cindexPrediction)
# 	pred=pred/(pred.max()-pred.min())
# 	pred-=pred.min()
# 	if offset=='auto':
# 		ms=take(shift(pred, -250), evts).sum()
# 		offset=-250
# 		for i in range(-249, 251):
# 			t=take(shift(pred, i), evts).sum()
# 			if t>ms:
# 				ms=t
# 				offset=i
# 		print offset, offset/ds.fs		
# 		pred=shift(pred, offset)
# 	elif offset:
# 		pred=shift(pred, int(offset*ds.fs))
# 	ds.assignChannel(cindexPrediction, pred)
# 	prob=0.0
# 	window=int(window*ds.fs)
# 	if window<2:
# 		pae=take(pred, evts)
# 		prob=pae.sum()
# 	else:	
# 		for e in evts:
# 			t=pred[e:e+window].mean()
# 			prob+=t
# 	pos=prob/len(evts)		
# 	neg=(pred.sum()-prob)/(len(pred)-len(evts)) 
# 	print pos, neg, pos-neg
# 	return ds
# 
# def lif(ds, cnameDP=0, cnameSpikes=-1, refract=1.0, leak=1.0, threshold=.5, rnoise=0):
# 	'''Use cnameDP as a driver potential for a leaky integrate and fire model. 
# 	Place the output spike train in cnameSpikes. 
# 		Refract: The amount to reduce the driver potential when an event
# 			occurs
# 		Leak: The "current" which returns the accumulated potential to
# 			0. Units are in exitation units per dP per second
# 		Threshold: The potential at which events trigger
# 		rnoise: The stdev of the distribution of Refract (default 0) '''
# 	leak=leak/ds.fs
# 	nei = []
# 	v=0
# 	vdp = ds.getChannel(cnameDP)/ds.fs
# 	storev = []
# 	for i in range(vdp.shape[0]):
# 		v+= vdp[i]
# 		v-= v*leak
# 		storev.append(v)
# 		if v>= threshold:
# 			nei.append(i)
# 			v-=refract
# 			if 	rnoise:
# 				v -= normal(0, rnoise)
# 	ind=ds.setChannel(cnameSpikes, zeros(ds.data.shape[0], ds.data.dtype.char))
# 	if len(nei)==0:
# 		storev = array(storev)
# 		print "No Events", storev.max(),  storev.mean()
# 	else:
# 		nei=array(nei)
# 		put(ds.data[:,ind], nei, 1.0)
# 		if len(nei)>1:
# 			delays=nei[1:]-nei[:-1]
# 		else:
# 			delays=array([0.0,1])
# 		print len(nei), delays.min(), 1.0/delays.mean()
# 	return ds
# 	
# 
# def spikeDistance(ds, tao=.005, cnameTrain1=2, cnameTrain2=3, sKey="Error"):
# 	'''convolve both spike trains with an exponential having decay time constant 
# 	tao, difference the trains, and enter the sqare root of sum of squared difference
# 	devided by tao in sKey'''
# 	filt=arange(int(ds.fs*tao*5))/ds.fs
# 	filt=exp(-filt/tao)
# 	if len(filt)<1:
# 		tao=1/ds.fs
# 		print "warning, tao is to short, using %.6f" % tao
# 		filt=ones(1)
# 	evts1=convolve(ds.getChannel(cnameTrain1), filt, mode=SAME)
# 	evts2=convolve(ds.getChannel(cnameTrain2), filt, mode=SAME)
# 	res=(evts1-evts2)**2
# 	norm= filt.sum()/ds.fs
# 	err=res.sum()/(ds.fs*norm)
# 	spikenum=ds.getChannel(cnameTrain1).sum()
# 	spikediff=ds.getChannel(cnameTrain1).sum() - ds.getChannel(cnameTrain2).sum()
# 	print "predicted %i spikes (of %i)" % (spikenum-spikediff, spikenum)
# 	print "Spike number error %i" % spikediff
# 	if abs(spikediff)>.3*spikenum:
# 		err+=abs(spikediff)
# 	print "error %.4f" % err
# 	ds.special[sKey]=err
# 	#ds.special[sKey]=abs(spikediff)
# 	return ds
# 
# def victorDistance(ds, cost,cnameTrain1=2, cnameTrain2=3, sKey="Error"):
# 	'''calculate the Victor/Purpura metric space spike distance'''
# 	evts1=ds.getChannel(cnameTrain1)
# 	evts1=nonzero(evts1)/ds.fs
# 	evts2=ds.getChannel(cnameTrain2)
# 	evts2=nonzero(evts2)/ds.fs
# 	
# 	nspi=len(evts1)
# 	nspj=len(evts2)
# 	if cost==0:
# 	   d=abs(nspi-nspj)
# 	elif cost==float('inf'):
# 	   d=nspi+nspj
# 	else:
# 		scr=zeros((nspi+1,nspj+1), Float32);
# 		scr[:,0]=arange(nspi+1)
# 		scr[0,:]=arange(nspj+1)
# 		if nspi and nspj:
# 		   for i in range(1,nspi+1):
# 			  for j in range(1,nspj+1):
# 				scr[i,j]=min([scr[i-1,j]+1,scr[i,j-1]+1, scr[i-1,j-1]+cost*abs(evts1[i-1]-evts2[j-1])])
# 		d=scr[nspi,nspj]
# 	print d
# 	ds.special[sKey]=d
# 	return ds
# 	
# 
# def stimSegmentTosKey(ds, xcoordStart=0.0, xcoordStop=.02, channels=[0], sKey="Kernel"):
# 	'''grab a segment of the stimulus in the indicated range, using the indicated channels,
# 	and copy it to the indicated special key (as a new dataset)'''
# 	ds2=ds.copy()
# 	ds2.crop(xcoordStart, xcoordStop)
# 	ds2.takeChannels(channels)
# 	ds.special[sKey]=ds2
# 	return ds
# 
# def combineDPs(ds, cindexOutput=-1, channels=[1,2,3], linco=3.0, difco=3.0, offset=.002, bits=4):
# 	'''Intended for use in optimization routines. linco and difco should range 
# 	from 0 to 2**(bits * (number of channels))-1 as integers. Constructs a combination of several 
# 	driver potentials (listed in channels). The combination is stored in cindexOutput. It is 
# 	computed as: 
# 	dp(t)=A0C0(t)+A1C1(t)+ ... AnCn(t) + (B0C0(t)-B0C0(t-offset)) ... (BnCn(t)-BnCn(t-offset))
# 	Where the Ci are the input channels, and the Ai and Bi are weighting coefficients determined 
# 	from the parameters linco (Ai) and difco (Bi). These are computed by taking the parameters
# 	modulo 2**bits, one at a time (A0 in computed from the most significant bits), and producing a 
# 	coefficient ranging from 0 to 1 in 2**bits steps.
# 	Output is normalized'''
# 	nc=len(channels)
# 	a=[]
# 	b=[]
# 	base=2**bits
# 	incr=1.0/(base-1)
# 	for i in range(nc):
# 		p=nc-i-1
# 		lcv, linco = divmod(linco, base**p)
# 		a.append(lcv*incr)
# 		dcv, difco=divmod(difco, base**p)
# 		b.append(dcv*incr)
# 	pred=zeros(ds.data.shape[0], ds.data.dtype.char)
# 	off=int(round(offset*ds.fs))
# 	for i in range(nc):
# 		chan=ds.getChannel(channels[i])
# 		cdt=shift(chan, off)
# 		cdt=chan-cdt
# 		pred+=chan*a[i]+cdt*b[i]
# 	pred=pred/pred.max()	
# 	ds.assignChannel(cindexOutput, pred)
# 	return ds
# 
# def insertKernels(ds, cnameTarget, cnameEvents="EventTimes", sKeyKernel="Kernel",
# 					latency=0.0, normalizeAmp=0.0, ampJitter=.10, 
# 					latencyJitter=3.0, stretchJitter=0.10, replace=0,
# 					smoothing=10):
# 	'''cnameEvents should be the name of a binary channel. The waveform found in 
# 	the special key sKeyTarget is inserted into channel cnameTarget at each event 
# 	time in cnameEvents. Latency is an offset in ms. Positive numbers indicate that
# 	the end of the inserted kernel occurs this many ms before the event time. 
# 	normalizeAmp, if nonzero, indicates that the kernel amylitude should be scaled 
# 	to the indicated fraction of the maximum value of the target channel. ampJitter, 
# 	if nonzero, indicates that the amplitude of subsequent inserts will be varried. The
# 	value is the standard deviation of the distribution of amplitudes expressed as a 
# 	fraction of the mean amplitude. latencyJitter, if nonzero, is the standard 
# 	deviation of the latencies in ms. StreatchJitter is the standard deviation of 
# 	temporal streatching as a fraction of the mean kernal length. If replace is 
# 	nonzero, the target channel is blancked in the region the kernel will fill
# 	before it is inserted. Smoothing is the number of points that are "warped" 
# 	on either side of an insert to prevent rough transitions.'''
# 	tindex=ds.labels.index(cnameTarget)
# 	dat=ds.channel(tindex)
# 	evts=ds.getEvents(ds.labels.index(cnameEvents), returnTimes=False)
# 	kern=ds.special[sKeyKernel]
# 	if normalizeAmp:
# 		kamp=kern.data.max()-kern.data.min()
# 		damp=dat.max()-dat.min()
# 		if damp!=0.0 and kamp!=0.0:
# 			kern.data=(kern.data/kamp)*damp*normalizeAmp
# 			ds.special[sKeyKernel]=kern
# 	kern=kern.data[:,0]
# 	if smoothing:
# 		sp=min(smoothing, int(.25*len(kern)))
# 		kern_template=kern[sp:-sp].copy()
# 	else:
# 		sp=0
# 	offset=(latency/-1000.0)*ds.fs-len(kern)
# 	newchan=dat.copy()
# 	nins = len(evts)
# 	if ampJitter:
# 		ampJ = normal(1.0, ampJitter, nins)
# 	else:
# 		ampJ = ones(nins)
# 	if latencyJitter:
# 		tsd = latencyJitter*ds.fs/1000.0
# 		tJ = normal(0, tsd, nins)
# 	else:
# 		tJ = zeros(nins)
# 	offset=tJ+offset
# 	if stretchJitter:
# 		stretch=normal(1.0, stretchJitter, nins)
# 	for i, ind in enumerate(evts):
# 		ind += offset[i]
# 		ind = int(ind)
# 		sind = ind+len(kern)
# 		if not 0<=ind:
# 			continue
# 		if not newchan.shape[0]>sind:
# 			break
# 		if sp:
# 			ins=kern_template.copy()
# 		else:
# 			ins=kern.copy()
# 		if stretchJitter:
# 			sf=max(.1, stretch[i])
# 			temp=ins
# 			ins=timestretch(ins, sf)
# 		if sp:
# 			if ind<2:
# 				lead=smoothConnect(array([0.0, 0.0]), ins, sp)	
# 			else:
# 				lead=smoothConnect(newchan[ind-2:ind], ins, sp)
# 			if sind>newchan.shape[0]-2:
# 				lag=smoothConnect(ins, array([0.0,0.0]), sp)
# 			else:
# 				lag=smoothConnect(ins, newchan[sind:sind+2], sp)
# 			ins=concatenate([lead, ins, lag])		
# 		if replace:
# 			newchan[ind:sind] = ins*ampJ[i]
# 		else:	
# 			newchan[ind:sind] = newchan[ind:sind]+ins*ampJ[i]
# 	ds.assignChannel(tindex, newchan)
# 	return ds
# 	

